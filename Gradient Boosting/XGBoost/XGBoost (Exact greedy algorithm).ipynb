{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af257a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2460c",
   "metadata": {},
   "source": [
    "# Hard-coded XGBoost for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7272f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps:\n",
    "# 1. Make initial prediction\n",
    "# 2. Compute Gradients and Hessians\n",
    "# 3. Fit into a new tree\n",
    "# 4. update the prediction\n",
    "# 5. Repeat the process form step 2 to step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12ad226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def XGBoostRegressionWithCoverPruning(X, y, learning_rate, num_iters, lambda_, lambda_l1=0.1, lambda_l2=0.1, pruning_threshold=0.01, min_cover=10):\n",
    "    '''\n",
    "    XGBoost function for regression tasks with pruning based on residuals and cover,\n",
    "    including L1 and L2 regularization\n",
    "    \n",
    "    X                        Features matrix\n",
    "    y                        Target\n",
    "    learning_rate            Model learning rate\n",
    "    num_iters                Number of boosting iterations\n",
    "    lambda_                  Regularization parameter\n",
    "    lambda_l1                L1 regularization strength (Lasso)\n",
    "    lambda_l2                L2 regularization strength (Ridge)\n",
    "    pruning_threshold        Threshold to stop further boosting (based on residuals)\n",
    "    min_cover                Minimum number of samples required in a leaf node\n",
    "    \n",
    "    F_x                      Cumulative prediction at each boosting step\n",
    "    gradients                First derivative of the loss function \n",
    "    hessians                 Second derivative of the loss function (Number of residuals)\n",
    "    h_x                      Prediction from each boosting step\n",
    "    '''\n",
    "    \n",
    "    F_x = np.mean(y) * np.ones_like(y)                  # First prediction\n",
    "    for i in range(num_iters):\n",
    "        residuals = y - F_x\n",
    "        gradients = -residuals                            # First derivative of the loss function\n",
    "        hessians = np.ones_like(y)                        # Second derivative (Hessian)\n",
    "        \n",
    "        # Apply L1 regularization to the gradients\n",
    "        gradients += lambda_l1 * np.sign(gradients)  # L1 regularization term\n",
    "        \n",
    "        # Apply L2 regularization to the hessians\n",
    "        hessians += lambda_l2 * np.ones_like(hessians)  # L2 regularization term (adding to the Hessian)\n",
    "        \n",
    "        # Fitting into new tree\n",
    "        h_x = -gradients / (hessians + lambda_)\n",
    "        \n",
    "        # Calculate the cover (number of samples per leaf)\n",
    "        cover = np.sum(np.abs(gradients) >= pruning_threshold)  # Simplified cover calculation\n",
    "        \n",
    "        # Check if pruning condition is met based on residuals and cover\n",
    "        if np.mean(np.abs(residuals)) < pruning_threshold or cover < min_cover:\n",
    "            print(f\"Pruning: Stopping boosting at iteration {i+1} due to low cover or small residuals\")\n",
    "            break\n",
    "        \n",
    "        F_x += learning_rate * h_x  # Updating cumulative prediction\n",
    "    \n",
    "    return F_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265fb96d",
   "metadata": {},
   "source": [
    "### Demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25386e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows predictions:\n",
      "Actual value: 0.6789030208597069        Prediction: 0.6140266633467152\n",
      "Actual value: 0.48640909899108653        Prediction: 0.4935603309753998\n",
      "Actual value: 0.16540090450135903        Prediction: 0.31157545577871276\n",
      "Actual value: 0.4705050789923272        Prediction: 0.4845440779816075\n",
      "Actual value: 0.785202867912825        Prediction: 0.6742898115511999\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
    "y = np.random.rand(1000)      # 1000 target values\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_iters = 100\n",
    "lambda_ = 1.0\n",
    "pruning_threshold = 0.01\n",
    "min_cover = 10  # Minimum samples per leaf\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_test_pred = XGBoostRegressionWithCoverPruning(X_test, y_test, learning_rate, num_iters, lambda_, pruning_threshold, min_cover)\n",
    "\n",
    "print(f\"First 5 rows predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Actual value: {y_test[i]}        Prediction: {y_test_pred[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c20542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error: 0.015090762389554011\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test performance\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test Mean Squared Error: {mse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c210f",
   "metadata": {},
   "source": [
    "# XGBoost for regression (sklean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0bd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620cbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,           # Number of boosting rounds\n",
    "    learning_rate=0.1,          # Learning rate\n",
    "    max_depth=3,                # Maximum depth of the trees (controls tree complexity)\n",
    "    min_child_weight=10,        # Minimum sum of weights for a child (prunes small leaves)\n",
    "    gamma=0.1,                  # Minimum loss reduction (controls pruning based on gain)\n",
    "    random_state=42             # Set random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eda12c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error: 0.08967959861851739\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test Mean Squared Error: {mse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78993f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE values are diffrent because sklean XGBoost model uses L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76ef66",
   "metadata": {},
   "source": [
    "# Hard-coded XGBoost for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20fed7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoostClassification(X, y, learning_rate, num_iters, lambda_):\n",
    "    '''\n",
    "    XGBoost function for binary classification tasks\n",
    "    \n",
    "    X                        Features matrix\n",
    "    y                        Target (binary labels, 0 or 1)\n",
    "    learning_rate            Model learning rate\n",
    "    num_iters                Number of boosting iterations\n",
    "    lambda_                  Regularization parameter\n",
    "    \n",
    "    F_x                      Cumulative prediction at each boosting step (log-odds)\n",
    "    gradients                First derivative of the loss function \n",
    "    hessians                 Second derivative of the loss function\n",
    "    h_x                      Prediction from each boosting step   \n",
    "    '''\n",
    "    \n",
    "    # Initialize predictions with log-odds \n",
    "    F_x = np.log(np.mean(y) / (1 - np.mean(y))) * np.ones_like(y)\n",
    "    log_odds_steps = [F_x.copy()]  # Track log-odds over iterations\n",
    "    probability_steps = [1 / (1 + np.exp(-F_x))]  # Track probabilities\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Convert log-odds to probability using the sigmoid function\n",
    "        preds = 1 / (1 + np.exp(-F_x))\n",
    "       \n",
    "        gradients = preds - y                              \n",
    "        hessians = preds * (1 - preds)                     \n",
    "        h_x = -gradients / (hessians + lambda_)            # Fitting new tree\n",
    "        F_x += learning_rate * h_x                         # Updating\n",
    "        \n",
    "        # Store intermediate log-odds and probabilities for visualization\n",
    "        log_odds_steps.append(F_x.copy())\n",
    "        probability_steps.append(1 / (1 + np.exp(-F_x)))\n",
    "    \n",
    "    # Convert final log-odds predictions to probabilities\n",
    "    final_preds = 1 / (1 + np.exp(-F_x))\n",
    "    return final_preds, log_odds_steps, probability_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158d9ad",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573b350d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0   1.125100   1.178124   0.493516   0.790880  -0.614278   1.347020   \n",
      "1  -0.564641   3.638629  -1.522415  -1.541705   1.616697   4.781310   \n",
      "2   0.516313   2.165426  -0.628486  -0.386923   0.492518   1.442381   \n",
      "3   0.537282   0.966618  -0.115420   0.670755  -0.958516   0.871440   \n",
      "4   0.278385   1.065828  -1.724917  -2.235667   0.715107   0.731249   \n",
      "\n",
      "   feature_7  feature_8  feature_9  feature_10  target  \n",
      "0   1.419515   1.357325   0.966041   -1.981139       1  \n",
      "1   3.190292  -0.890254   1.438826   -3.828748       0  \n",
      "2   1.332905  -1.958175  -0.348803   -1.804124       0  \n",
      "3   0.508186  -1.034471  -1.654176   -1.910503       1  \n",
      "4  -0.674119   0.598330  -0.524283    1.047610       0  \n"
     ]
    }
   ],
   "source": [
    "# Parameters for dataset generation\n",
    "n_samples = 1000         # Number of samples\n",
    "n_features = 10          # Total number of features\n",
    "n_informative = 5        # Number of informative features\n",
    "n_redundant = 2          # Number of redundant features\n",
    "n_classes = 2            # Number of classes (binary classification)\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_informative,\n",
    "    n_redundant=n_redundant,\n",
    "    n_classes=n_classes,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "data = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(n_features)])\n",
    "data['target'] = y\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a20a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows predictions:\n",
      "Actual value: 0        Prediction: 0\n",
      "Actual value: 1        Prediction: 1\n",
      "Actual value: 0        Prediction: 0\n",
      "Actual value: 0        Prediction: 0\n",
      "Actual value: 0        Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_iters = 100\n",
    "lambda_ = 1.0\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_test_pred, log_odds_steps, probability_steps = XGBoostClassification(X_test, y_test, learning_rate, num_iters, lambda_)\n",
    "\n",
    "# Assuming threshhold is 0.5\n",
    "y_test_pred_class = np.where(y_test_pred >= 0.5, 1, 0)     \n",
    "\n",
    "# Display first 5 actual values and predictions, and print accuracy\n",
    "print(\"First 5 rows predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Actual value: {y_test[i]}        Prediction: {y_test_pred_class[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95de5c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 1.00\n",
      "Test Mean Squared Error: 0.013\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test performance\n",
    "accuracy = accuracy_score(y_test, y_test_pred_class)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Test Mean Squared Error: {mse_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa945085",
   "metadata": {},
   "source": [
    "# XGBoost for classification (sklean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d82f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4fde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be8bf675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.95\n",
      "Test Mean Squared Error: 0.045\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance using accuracy\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {accuracy_test:.2f}\")\n",
    "# Evaluate the test performance\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test Mean Squared Error: {mse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229b456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
